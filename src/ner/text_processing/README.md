### Tools
- `mytokenizer.py` (plus the files in /lib) is [YeDeheng's tokenizer](https://github.com/YeDeheng/s-tokenizer) modified for Python 3.6.5.
- `tokenizer.py` is our customized [spaCy](https://spacy.io/) tokenizer for corpus preprocessing and sentence splitting for text containing software terms (mainly for Java).
- `corpus_utils.py` provides a class of utility functions for sentence splitting, text preprocessing and crf model training.
